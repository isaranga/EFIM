1. During algorithm 1, after step 5 (removing items that are not in secondary
because those items cannot be in any high-utility pattern), LU's of all items are
probably changed (can only change downwards). Then, some of them may be lower than
minutil, but they are not recalculated during the algorithm. Does it change anything
if they are recalculated before step 7 (calculation of SU) and forward steps?
Let's try to put steps 2-5 in a loop, having new unpromising items removed over and over
(and order of items reevaluated), until there aren't LU's lower than minutil.
It can be proved easily that items with "new" LU lower than minutil are unpromising and
can be removed from dataset, experiments will show if it helps performance.

2. Following the same idea of recalculating LU and SU after unpromising items are removed
from transaction, same can be done in algorithm 2. We put steps 5-7 in a loop, and add step 7.5
(after 7 and before 8) that called minimizing dataset. We check each transaction in beta-D and remove
from it any items that are not in "secondary", meaning they can not be a part of high utility pattern
in this branch of the tree anyway. Then we go back to step 5. Using that, we can get a final group of
"primary" items that is smaller if we don't recalculate those boundaries. We stop the loop when length
of new_secondary is no smaller than the length of previous secondary set, meaning we can't minimize
the dataset further.